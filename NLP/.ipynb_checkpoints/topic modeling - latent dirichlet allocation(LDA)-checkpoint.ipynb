{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation, LDA\n",
    "- https://wikidocs.net/30708\n",
    "- 토픽의 개수(k) 결정\n",
    "- 모든 단어를 k개 중 하나의 토픽에 랜덤으로 할당\n",
    "- 이제 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행\n",
    "    - 어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당되어져 있지만, 다른 단어들은 전부 올바른 토픽에 할당되어져 있는 상태라고 가정합니다. 이에 따라 단어 w는 아래의 두 가지 기준에 따라서 토픽이 재할당됩니다.\n",
    "        - p(topic t | document d) : 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율\n",
    "        - p(word w | topic t) : 단어 w를 갖고 있는 모든 문서들 중 토픽 t가 할당된 비율\n",
    "\n",
    "잠재 디리클레 할당과 잠재 의미 분석의 차이\n",
    "- LSA : DTM을 차원 축소 하여 축소 차원에서 근접 단어들을 토픽으로 묶는다.\n",
    "- LDA : 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:02:23.971985Z",
     "start_time": "2020-09-03T14:02:21.758241Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# !pip install pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - use gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:02:25.109391Z",
     "start_time": "2020-09-03T14:02:23.973705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:02:26.236557Z",
     "start_time": "2020-09-03T14:02:25.111421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure about story seem biased what disagre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>yeah expect people read actually accept hard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>although realize that principle your strongest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>notwithstanding legitimate fuss about this pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>well will have change scoring playoff pool unf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  Well i'm not sure about the story nad it did s...   \n",
       "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "2  Although I realize that principle is not one o...   \n",
       "3  Notwithstanding all the legitimate fuss about ...   \n",
       "4  Well, I will have to change the scoring on my ...   \n",
       "\n",
       "                                      clean_document  \n",
       "0  well sure about story seem biased what disagre...  \n",
       "1  yeah expect people read actually accept hard a...  \n",
       "2  although realize that principle your strongest...  \n",
       "3  notwithstanding legitimate fuss about this pro...  \n",
       "4  well will have change scoring playoff pool unf...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"document\":documents})\n",
    "# 알파벳만 남기고 제거\n",
    "df[\"clean_document\"] = df[\"document\"].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어 제거 > 왱? 짧은 단어는 유용한 정보를 담고있지 않다고 가정한다는군\n",
    "df[\"clean_document\"] = df[\"clean_document\"].apply(lambda x: \" \".join([w for w in x.split() if len(w) > 3]))\n",
    "# 소문자로 변환\n",
    "df[\"clean_document\"] = df[\"clean_document\"].apply(lambda x: x.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:02:26.243869Z",
     "start_time": "2020-09-03T14:02:26.238456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stopwords = stopwords.words('english')\n",
    "len(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:02:28.324695Z",
     "start_time": "2020-09-03T14:02:26.245595Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well',\n",
       " 'sure',\n",
       " 'about',\n",
       " 'story',\n",
       " 'seem',\n",
       " 'biased',\n",
       " 'what',\n",
       " 'disagree',\n",
       " 'with',\n",
       " 'your',\n",
       " 'statement',\n",
       " 'that',\n",
       " 'media',\n",
       " 'ruin',\n",
       " 'israels',\n",
       " 'reputation',\n",
       " 'that',\n",
       " 'rediculous',\n",
       " 'media',\n",
       " 'most',\n",
       " 'israeli',\n",
       " 'media',\n",
       " 'world',\n",
       " 'having',\n",
       " 'lived',\n",
       " 'europe',\n",
       " 'realize',\n",
       " 'that',\n",
       " 'incidences',\n",
       " 'such',\n",
       " 'described',\n",
       " 'letter',\n",
       " 'have',\n",
       " 'occured',\n",
       " 'media',\n",
       " 'whole',\n",
       " 'seem',\n",
       " 'ignore',\n",
       " 'them',\n",
       " 'subsidizing',\n",
       " 'israels',\n",
       " 'existance',\n",
       " 'europeans',\n",
       " 'least',\n",
       " 'same',\n",
       " 'degree',\n",
       " 'think',\n",
       " 'that',\n",
       " 'might',\n",
       " 'reason',\n",
       " 'they',\n",
       " 'report',\n",
       " 'more',\n",
       " 'clearly',\n",
       " 'atrocities',\n",
       " 'what',\n",
       " 'shame',\n",
       " 'that',\n",
       " 'austria',\n",
       " 'daily',\n",
       " 'reports',\n",
       " 'inhuman',\n",
       " 'acts',\n",
       " 'commited',\n",
       " 'israeli',\n",
       " 'soldiers',\n",
       " 'blessing',\n",
       " 'received',\n",
       " 'from',\n",
       " 'government',\n",
       " 'makes',\n",
       " 'some',\n",
       " 'holocaust',\n",
       " 'guilt',\n",
       " 'away',\n",
       " 'after',\n",
       " 'look',\n",
       " 'jews',\n",
       " 'treating',\n",
       " 'other',\n",
       " 'races',\n",
       " 'when',\n",
       " 'they',\n",
       " 'power',\n",
       " 'unfortunate']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokenize_document\"] = df[\"clean_document\"].apply(lambda x: [w for w in x.split() if x not in english_stopwords])\n",
    "df[\"tokenize_document\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정수 인코딩, 단어 집합 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:11:58.583302Z",
     "start_time": "2020-09-03T14:11:56.771262Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(df[\"tokenize_document\"])\n",
    "corpus = [dictionary.doc2bow(text) for text in df[\"tokenize_document\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:12:20.862584Z",
     "start_time": "2020-09-03T14:12:20.858857Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 7),\n",
       " (2, 1),\n",
       " (5, 1),\n",
       " (14, 1),\n",
       " (17, 57),\n",
       " (20, 12),\n",
       " (21, 2),\n",
       " (29, 2),\n",
       " (30, 2),\n",
       " (32, 5)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[10][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:13:07.480355Z",
     "start_time": "2020-09-03T14:13:07.476546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64365, 'about')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary), dictionary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:17:47.613980Z",
     "start_time": "2020-09-03T14:16:16.500724Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 20\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus, \n",
    "    num_topics=NUM_TOPICS,\n",
    "    id2word=dictionary,\n",
    "    passes=15 #알고리즘 동작횟수\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:17:49.525614Z",
     "start_time": "2020-09-03T14:17:49.509789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.037*\"window\" + 0.021*\"display\" + 0.014*\"widget\" + 0.014*\"mouse\" + 0.013*\"color\"')\n",
      "(1, '0.014*\"bike\" + 0.011*\"engine\" + 0.011*\"cars\" + 0.009*\"than\" + 0.007*\"road\"')\n",
      "(2, '0.017*\"master\" + 0.010*\"plane\" + 0.010*\"slave\" + 0.008*\"allah\" + 0.007*\"part\"')\n",
      "(3, '0.017*\"game\" + 0.015*\"team\" + 0.013*\"year\" + 0.013*\"will\" + 0.012*\"games\"')\n",
      "(4, '0.049*\"that\" + 0.020*\"this\" + 0.015*\"have\" + 0.011*\"with\" + 0.010*\"what\"')\n",
      "(5, '0.025*\"guns\" + 0.016*\"control\" + 0.015*\"firearms\" + 0.015*\"crime\" + 0.012*\"weapons\"')\n",
      "(6, '0.018*\"were\" + 0.013*\"their\" + 0.012*\"from\" + 0.010*\"that\" + 0.009*\"they\"')\n",
      "(7, '0.019*\"that\" + 0.016*\"this\" + 0.016*\"will\" + 0.012*\"with\" + 0.010*\"government\"')\n",
      "(8, '0.026*\"with\" + 0.024*\"have\" + 0.020*\"this\" + 0.017*\"that\" + 0.009*\"would\"')\n",
      "(9, '0.009*\"overtime\" + 0.009*\"arafat\" + 0.007*\"bird\" + 0.006*\"padded\" + 0.005*\"brilliant\"')\n",
      "(10, '0.019*\"scores\" + 0.015*\"maine\" + 0.012*\"mydisplay\" + 0.011*\"stats\" + 0.008*\"joystick\"')\n",
      "(11, '0.006*\"cross\" + 0.006*\"mary\" + 0.005*\"scsi\" + 0.005*\"byte\" + 0.004*\"main\"')\n",
      "(12, '0.050*\"space\" + 0.019*\"nasa\" + 0.011*\"launch\" + 0.010*\"earth\" + 0.009*\"satellite\"')\n",
      "(13, '0.023*\"channel\" + 0.012*\"radio\" + 0.012*\"speaker\" + 0.010*\"batteries\" + 0.010*\"channels\"')\n",
      "(14, '0.013*\"file\" + 0.013*\"from\" + 0.012*\"this\" + 0.009*\"with\" + 0.009*\"your\"')\n",
      "(15, '0.046*\"that\" + 0.030*\"they\" + 0.019*\"have\" + 0.015*\"this\" + 0.013*\"there\"')\n",
      "(16, '0.027*\"water\" + 0.026*\"ground\" + 0.016*\"wiring\" + 0.014*\"nist\" + 0.012*\"picture\"')\n",
      "(17, '0.021*\"health\" + 0.018*\"university\" + 0.016*\"medical\" + 0.012*\"research\" + 0.012*\"april\"')\n",
      "(18, '0.027*\"period\" + 0.017*\"power\" + 0.015*\"play\" + 0.010*\"scorer\" + 0.010*\"puck\"')\n",
      "(19, '0.029*\"sale\" + 0.024*\"price\" + 0.023*\"offer\" + 0.023*\"shipping\" + 0.020*\"condition\"')\n"
     ]
    }
   ],
   "source": [
    "# 각 토픽별 n개의 단어 기여도\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:22:21.260549Z",
     "start_time": "2020-09-03T14:18:40.344010Z"
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 별 토픽 분포\n",
    "토픽 번호와 해당 토픽이 해당 문서에서 차지하는 분포도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:22:30.147354Z",
     "start_time": "2020-09-03T14:22:30.130999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 문서의 topic 비율은 [(0, 0.04549789), (1, 0.027804269), (4, 0.49468353), (6, 0.42247725)]\n",
      "1번째 문서의 topic 비율은 [(3, 0.11492364), (4, 0.50609195), (8, 0.11009235), (15, 0.23139717), (16, 0.021851415)]\n",
      "2번째 문서의 topic 비율은 [(4, 0.0679482), (6, 0.22659662), (7, 0.17762864), (15, 0.4937417), (17, 0.024774402)]\n",
      "3번째 문서의 topic 비율은 [(2, 0.033558078), (4, 0.20757632), (7, 0.316315), (8, 0.06129396), (14, 0.07817704), (15, 0.29518023)]\n",
      "4번째 문서의 topic 비율은 [(3, 0.69520897), (4, 0.2766572)]\n"
     ]
    }
   ],
   "source": [
    "for i, topic_list in enumerate(lda_model[corpus]):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(f\"{i}번째 문서의 topic 비율은 {topic_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:30:20.692083Z",
     "start_time": "2020-09-03T14:30:20.686533Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_topictable_per_doc(ldamodel, corpus):\n",
    "    topic_table = pd.DataFrame()\n",
    "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
    "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(doc):\n",
    "            if j == 0:\n",
    "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    return(topic_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T14:30:46.551052Z",
     "start_time": "2020-09-03T14:30:21.529973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문서 번호</th>\n",
       "      <th>가장 비중이 높은 토픽</th>\n",
       "      <th>가장 높은 토픽의 비중</th>\n",
       "      <th>각 토픽의 비중</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>[(0, 0.04549803), (1, 0.027804693), (4, 0.4946...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>[(3, 0.11491867), (4, 0.50600535), (8, 0.11010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.4918</td>\n",
       "      <td>[(4, 0.07055347), (6, 0.22636026), (7, 0.17720...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.3163</td>\n",
       "      <td>[(2, 0.033558078), (4, 0.20754296), (7, 0.3163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6952</td>\n",
       "      <td>[(3, 0.6952174), (4, 0.27664882)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.3843</td>\n",
       "      <td>[(2, 0.32366607), (3, 0.043017816), (4, 0.3842...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.6058</td>\n",
       "      <td>[(3, 0.051869333), (4, 0.17554958), (7, 0.0121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.3761</td>\n",
       "      <td>[(4, 0.28630856), (6, 0.37614602), (15, 0.3261...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.4655</td>\n",
       "      <td>[(4, 0.27312687), (7, 0.09122808), (11, 0.0282...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.3960</td>\n",
       "      <td>[(1, 0.14238901), (4, 0.08519253), (6, 0.07451...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   문서 번호  가장 비중이 높은 토픽  가장 높은 토픽의 비중  \\\n",
       "0      0           4.0        0.4947   \n",
       "1      1           4.0        0.5060   \n",
       "2      2          15.0        0.4918   \n",
       "3      3           7.0        0.3163   \n",
       "4      4           3.0        0.6952   \n",
       "5      5           4.0        0.3843   \n",
       "6      6           8.0        0.6058   \n",
       "7      7           6.0        0.3761   \n",
       "8      8          15.0        0.4655   \n",
       "9      9          15.0        0.3960   \n",
       "\n",
       "                                            각 토픽의 비중  \n",
       "0  [(0, 0.04549803), (1, 0.027804693), (4, 0.4946...  \n",
       "1  [(3, 0.11491867), (4, 0.50600535), (8, 0.11010...  \n",
       "2  [(4, 0.07055347), (6, 0.22636026), (7, 0.17720...  \n",
       "3  [(2, 0.033558078), (4, 0.20754296), (7, 0.3163...  \n",
       "4                  [(3, 0.6952174), (4, 0.27664882)]  \n",
       "5  [(2, 0.32366607), (3, 0.043017816), (4, 0.3842...  \n",
       "6  [(3, 0.051869333), (4, 0.17554958), (7, 0.0121...  \n",
       "7  [(4, 0.28630856), (6, 0.37614602), (15, 0.3261...  \n",
       "8  [(4, 0.27312687), (7, 0.09122808), (11, 0.0282...  \n",
       "9  [(1, 0.14238901), (4, 0.08519253), (6, 0.07451...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topictable = make_topictable_per_doc(lda_model, corpus)\n",
    "topictable = topictable.reset_index()\n",
    "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n",
    "topictable[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise2 - use sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T23:52:49.926869Z",
     "start_time": "2020-09-03T23:52:49.920046Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "약 15년 동안 발행되었던 뉴스 기사 제목을 모아놓은 영어 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T00:08:17.471087Z",
     "start_time": "2020-09-04T00:08:12.151164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1082168, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\", filename=\"abcnews-date-text.csv\")\n",
    "df = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T00:08:17.498138Z",
     "start_time": "2020-09-04T00:08:17.473510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1082168"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df[[\"headline_text\"]]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "- tokenize\n",
    "- remove stopwords\n",
    "- apply lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T00:08:17.506206Z",
     "start_time": "2020-09-04T00:08:17.501147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopwords = stopwords.words(\"english\")\n",
    "len(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T00:10:38.625690Z",
     "start_time": "2020-09-04T00:08:17.509337Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/user/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/user/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aba, decide, community, broadcast, licence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[act, fire, witness, must, aware, defamation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[g, call, infrastructure, protection, summit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[air, nz, staff, aust, strike, pay, rise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[air, nz, strike, affect, australian, travellers]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0       [aba, decide, community, broadcast, licence]\n",
       "1      [act, fire, witness, must, aware, defamation]\n",
       "2      [g, call, infrastructure, protection, summit]\n",
       "3          [air, nz, staff, aust, strike, pay, rise]\n",
       "4  [air, nz, strike, affect, australian, travellers]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[\"headline_text\"] = text.apply(lambda row: nltk.word_tokenize(row[\"headline_text\"]), axis=1)\n",
    "text[\"headline_text\"] = text[\"headline_text\"].apply(lambda x: [w for w in x if w not in en_stopwords])\n",
    "# 표제어 추출로 3인칭 단수 표현을 1인칭으로 바꾸고, 과거 현재형 동사를 현재형으로 바꿉니다.\n",
    "text[\"headline_text\"] = text[\"headline_text\"].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos=\"v\") for word in x])\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T00:10:41.133981Z",
     "start_time": "2020-09-04T00:10:38.628411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [decide, community, broadcast, licence]\n",
       "1      [fire, witness, must, aware, defamation]\n",
       "2    [call, infrastructure, protection, summit]\n",
       "3                   [staff, aust, strike, rise]\n",
       "4      [strike, affect, australian, travellers]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc = text[\"headline_text\"].apply(lambda x: [word for word in x if len(word) > 3])\n",
    "tokenized_doc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T00:11:16.582201Z",
     "start_time": "2020-09-04T00:10:41.135663Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decide community broadcast licence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fire witness must aware defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>call infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>staff aust strike rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>strike affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           headline_text\n",
       "0     decide community broadcast licence\n",
       "1     fire witness must aware defamation\n",
       "2  call infrastructure protection summit\n",
       "3                 staff aust strike rise\n",
       "4    strike affect australian travellers"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = \" \".join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "    \n",
    "text[\"headline_text\"] = detokenized_doc\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T00:11:22.955634Z",
     "start_time": "2020-09-04T00:11:16.590756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082168, 1000)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000)\n",
    "X = vectorizer.fit_transform(text[\"headline_text\"])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T00:14:02.195527Z",
     "start_time": "2020-09-04T00:11:46.099946Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=10,\n",
    "    learning_method=\"online\",\n",
    "    random_state=0,\n",
    "    max_iter=1\n",
    ")\n",
    "lda_top=lda_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T00:14:02.201709Z",
     "start_time": "2020-09-04T00:14:02.197514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.51600412e+02 1.00001494e-01 1.00005049e-01 ... 1.00004264e-01\n",
      "  1.00003474e-01 1.00004800e-01]\n",
      " [1.00001272e-01 1.00001222e-01 1.00003570e-01 ... 1.00008733e-01\n",
      "  1.00001628e-01 1.00003957e-01]\n",
      " [1.00001823e-01 1.00000548e-01 1.00001147e-01 ... 1.00003255e-01\n",
      "  1.00003647e-01 1.00003533e-01]\n",
      " ...\n",
      " [1.00002890e-01 1.13513398e+03 1.00017071e-01 ... 1.00005619e-01\n",
      "  1.00002781e-01 1.00002958e-01]\n",
      " [1.00001584e-01 1.00001246e-01 1.00004508e-01 ... 1.00004064e-01\n",
      "  1.00002811e-01 7.53381835e+02]\n",
      " [1.00002401e-01 1.00002691e-01 1.00015040e-01 ... 1.77619511e+03\n",
      "  1.50652739e+02 1.00004547e-01]]\n",
      "(10, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.components_)\n",
    "print(lda_model.components_.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T00:19:10.486445Z",
     "start_time": "2020-09-04T00:19:10.479123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('east', 7561.63), ('record', 6758.36), ('reporter', 5658.99), ('teacher', 5611.33), ('western', 5429.41)]\n",
      "Topic 2: [('protect', 13691.08), ('press', 7528.43), ('super', 5053.45), ('cyclone', 4610.97), ('year', 4546.98)]\n",
      "Topic 3: [('team', 12092.44), ('king', 8725.19), ('likely', 6113.49), ('pilot', 5851.6), ('howard', 5674.38)]\n",
      "Topic 4: [('check', 5935.06), ('push', 5874.27), ('tsunami', 5586.42), ('surprise', 4533.32), ('affect', 4070.65)]\n",
      "Topic 5: [('whale', 6677.03), ('witness', 5488.19), ('newcastle', 5225.56), ('legal', 4693.03), ('declare', 3652.78)]\n",
      "Topic 6: [('heritage', 11088.95), ('gaza', 8428.8), ('video', 8393.29), ('expand', 6268.13), ('horse', 5663.0)]\n",
      "Topic 7: [('help', 6959.64), ('refuse', 5924.98), ('time', 5834.63), ('hurt', 5545.86), ('answer', 5465.06)]\n",
      "Topic 8: [('carbon', 6707.7), ('south', 6456.53), ('issue', 6112.23), ('tourism', 5488.62), ('japanese', 5251.55)]\n",
      "Topic 9: [('commissioner', 11966.41), ('marine', 5502.89), ('korea', 5142.38), ('august', 4232.53), ('loss', 4223.4)]\n",
      "Topic 10: [('music', 7720.12), ('john', 5115.01), ('bombers', 4269.85), ('city', 4121.07), ('residents', 4038.68)]\n"
     ]
    }
   ],
   "source": [
    "# terms = vectorizer.get_feature_nemas()\n",
    "terms = list(vectorizer.vocabulary_.keys()) # 이게 맞는건가?\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(lda_model.components_,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
