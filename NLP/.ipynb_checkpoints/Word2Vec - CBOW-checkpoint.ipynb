{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec - CBOW\n",
    "- CBOW(Continuous Bag of Words Model)\n",
    "- batch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100\n",
    "window = 5\n",
    "min_count = 5\n",
    "negative = 5\n",
    "sg = 0\n",
    "ns_exponent = 0.75\n",
    "cbow_mean = 1\n",
    "alpha = 0.025# initial learning rate\n",
    "# max_vocab_size = \n",
    "# max_final_vocab = \n",
    "# sample = \n",
    "iter = 10\n",
    "# batch_words = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "- IMDB 영화 리뷰 데이터\n",
    "    - 텍스트 분류 중 감정 분류 모델 연습용으로 쓰임\n",
    "    - Label: 영화 리뷰가 긍정이면 1, 부정이면 0\n",
    "- word embedding을 생성하는데 적절한 데이터는 아닌것 같지만 임시로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 3s 0us/step\n",
      "X_train: (25000,), y_train: (25000,)\n",
      "X_test: (25000,), y_test: (25000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(\n",
    "    num_words=None, # max number of words to include\n",
    "    skip_top=0, # skip the top N most frequently occurring words\n",
    ")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of categories: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"The num of categories: {len(set(y_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of words: 88584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "print(f\"The num of words: {len(test2index)}\")\n",
    "test2index[\"woods\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = \n",
    "window = \n",
    "min_count = \n",
    "negative = \n",
    "sg = \n",
    "ns_exponent = \n",
    "cbow_mean = \n",
    "alpha = # initial learning rate\n",
    "max_vocab_size = # 그냥 corpus에 있는 전체 단어 수, 최댓값\n",
    "max_final_vocab = # min_count를 적용한후 남는 단어 수\n",
    "sample = \n",
    "iter = \n",
    "batch_words = \n",
    "\n",
    "X = \n",
    "Y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.Variable https://www.tensorflow.org/api_docs/python/tf/Variable?hl=en\n",
    "W = tf.Variable(\n",
    "    initial_value=\"normal\",\n",
    "    trainable=True,\n",
    "    shape=(max_final_vocab, size),\n",
    "    dtype=tf.float32,\n",
    "    name=\"W\",\n",
    ")\n",
    "U = tf.Variable(\n",
    "    initial_value=\"normal\",\n",
    "    trainable=True,\n",
    "    shape=(size, max_final_vocab),\n",
    "    dtype=tf.float32,\n",
    "    name=\"U\",\n",
    ")\n",
    "\n",
    "input = X.... # shape=(2*window, N)\n",
    "w = # shape=(2*window, N)\n",
    "u = \n",
    "\n",
    "# \n",
    "embedded_word_vectors = tf.matmul(w, input)\n",
    "hidden_layer = tf.cond(\n",
    "    cbow_mean == 1, \n",
    "    lambda: tf.math.reduce_mean(\n",
    "        embedded_word_vectors, \n",
    "        axis=0,\n",
    "        name=\"embedded_word_vector\"\n",
    "    ),\n",
    "    lambda: tf.math.reduce_sum(\n",
    "        embedded_word_vectors, \n",
    "        axis=0,\n",
    "        name=\"embedded_word_vector\"\n",
    "    ),\n",
    ")\n",
    "output_layer = tf.matmul(hidden_layer, u)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
