{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 잠재 의미 분석(Latent Semantic Analysis, LSA)\n",
    "- https://wikidocs.net/24949\n",
    "- LSA는 기본적으로 DTM이나 TF-IDF 행렬에 절단된 SVD(truncated SVD)를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어낸다는 아이디어를 갖고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:22:12.075525Z",
     "start_time": "2020-09-02T13:22:11.277320Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD, Truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:41:26.584669Z",
     "start_time": "2020-09-02T12:41:26.574300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [0,0,0,1,0,1,1,0,0],\n",
    "    [0,0,0,1,1,0,1,0,0],\n",
    "    [0,1,1,0,2,0,0,0,0],\n",
    "    [1,0,0,0,0,0,0,1,1]\n",
    "])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:44:43.415053Z",
     "start_time": "2020-09-02T12:44:43.404671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n",
       "doc1    0   0   0   1    0   1   1   0    0\n",
       "doc2    0   0   0   1    1   0   1   0    0\n",
       "doc3    0   1   1   0    2   0   0   0    0\n",
       "doc4    1   0   0   0    0   0   0   1    1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    A, \n",
    "    columns=[\"과일이\", \"길고\", \"노란\", \"먹고\", \"바나나\", \"사과\", \"싶은\", \"저는\", \"좋아요\"],\n",
    "    index=[\"doc1\", \"doc2\", \"doc3\", \"doc4\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:46:24.712646Z",
     "start_time": "2020-09-02T12:46:24.699286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 4), (4,), (9, 9))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, s, V = np.linalg.svd(A, full_matrices=True)\n",
    "U.shape, s.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:46:57.990092Z",
     "start_time": "2020-09-02T12:46:57.984869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.24,  0.75,  0.  , -0.62],\n",
       "        [-0.51,  0.44, -0.  ,  0.74],\n",
       "        [-0.83, -0.49, -0.  , -0.27],\n",
       "        [-0.  , -0.  ,  1.  ,  0.  ]]),\n",
       " array([2.69, 2.05, 1.73, 0.77]),\n",
       " array([[-0.  , -0.31, -0.31, -0.28, -0.8 , -0.09, -0.28, -0.  , -0.  ],\n",
       "        [ 0.  , -0.24, -0.24,  0.58, -0.26,  0.37,  0.58, -0.  , -0.  ],\n",
       "        [ 0.58, -0.  ,  0.  ,  0.  , -0.  ,  0.  , -0.  ,  0.58,  0.58],\n",
       "        [ 0.  , -0.35, -0.35,  0.16,  0.25, -0.8 ,  0.16, -0.  , -0.  ],\n",
       "        [-0.  , -0.78, -0.01, -0.2 ,  0.4 ,  0.4 , -0.2 ,  0.  ,  0.  ],\n",
       "        [-0.29,  0.31, -0.78, -0.24,  0.23,  0.23,  0.01,  0.14,  0.14],\n",
       "        [-0.29, -0.1 ,  0.26, -0.59, -0.08, -0.08,  0.66,  0.14,  0.14],\n",
       "        [-0.5 , -0.06,  0.15,  0.24, -0.05, -0.05, -0.19,  0.75, -0.25],\n",
       "        [-0.5 , -0.06,  0.15,  0.24, -0.05, -0.05, -0.19, -0.25,  0.75]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.round(2), s.round(2), V.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:48:10.563635Z",
     "start_time": "2020-09-02T12:48:10.558899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0000000000000002, 1.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# orhogonal matrix\n",
    "sum(U[0] * U[0]), sum(U[1] * U[1]), sum(U[2] * U[2]), sum(U[3] * U[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:50:20.401787Z",
     "start_time": "2020-09-02T12:50:20.397380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.69, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 2.05, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.73, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.77, 0.  , 0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diagonal matrix\n",
    "S = np.zeros(A.shape)\n",
    "S[:4, :4] = np.diag(s)\n",
    "S.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:51:13.440507Z",
     "start_time": "2020-09-02T12:51:13.436311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.48519247e-17,  5.62821277e-17,  5.62821277e-17,\n",
       "         1.00000000e+00, -2.41172566e-17,  1.00000000e+00,\n",
       "         1.00000000e+00,  5.48519247e-17,  5.48519247e-17],\n",
       "       [ 2.53218958e-16, -4.34272465e-17, -4.34272465e-17,\n",
       "         1.00000000e+00,  1.00000000e+00, -1.70971093e-16,\n",
       "         1.00000000e+00, -8.22778870e-17, -8.22778870e-17],\n",
       "       [ 1.07780421e-16,  1.00000000e+00,  1.00000000e+00,\n",
       "        -1.57145142e-16,  2.00000000e+00, -1.68949211e-16,\n",
       "        -1.57145142e-16,  6.85649058e-17,  6.85649058e-17],\n",
       "       [ 1.00000000e+00, -3.37739028e-17,  2.08965714e-16,\n",
       "         3.73418078e-16, -2.43289974e-17,  2.43959150e-16,\n",
       "        -1.11744589e-17,  1.00000000e+00,  1.00000000e+00]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_A = np.dot(np.dot(U, S), V)\n",
    "_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:51:22.807123Z",
     "start_time": "2020-09-02T12:51:22.803477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(A, _A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:54:47.364428Z",
     "start_time": "2020-09-02T12:54:47.359670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24,  0.75],\n",
       "       [-0.51,  0.44],\n",
       "       [-0.83, -0.49],\n",
       "       [-0.  , -0.  ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_U = U[:, :2]\n",
    "trunc_U.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:54:13.848466Z",
     "start_time": "2020-09-02T12:54:13.844705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.69, 0.  ],\n",
       "       [0.  , 2.05]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_S = S[:2, :2]\n",
    "trunc_S.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:55:08.488455Z",
     "start_time": "2020-09-02T12:55:08.484652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.  , -0.31, -0.31, -0.28, -0.8 , -0.09, -0.28, -0.  , -0.  ],\n",
       "       [ 0.  , -0.24, -0.24,  0.58, -0.26,  0.37,  0.58, -0.  , -0.  ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_V = V[:2, :]\n",
    "trunc_V.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:55:52.687542Z",
     "start_time": "2020-09-02T12:55:52.683077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  , -0.17, -0.17,  1.08,  0.12,  0.62,  1.08, -0.  , -0.  ],\n",
       "       [ 0.  ,  0.2 ,  0.2 ,  0.91,  0.86,  0.45,  0.91,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.93,  0.93,  0.03,  2.05, -0.17,  0.03,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.  ,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_hat = np.dot(np.dot(trunc_U, trunc_S), trunc_V)\n",
    "A_hat.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:56:03.091574Z",
     "start_time": "2020-09-02T12:56:03.088079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(A, A_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:58:58.062626Z",
     "start_time": "2020-09-02T12:58:35.583060Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T12:58:58.067042Z",
     "start_time": "2020-09-02T12:58:58.064343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:00:41.214933Z",
     "start_time": "2020-09-02T13:00:41.210802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# category for article\n",
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:04:52.446535Z",
     "start_time": "2020-09-02T13:04:51.328589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure about story seem biased what disagre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>yeah expect people read actually accept hard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>although realize that principle your strongest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>notwithstanding legitimate fuss about this pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>well will have change scoring playoff pool unf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  Well i'm not sure about the story nad it did s...   \n",
       "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "2  Although I realize that principle is not one o...   \n",
       "3  Notwithstanding all the legitimate fuss about ...   \n",
       "4  Well, I will have to change the scoring on my ...   \n",
       "\n",
       "                                      clean_document  \n",
       "0  well sure about story seem biased what disagre...  \n",
       "1  yeah expect people read actually accept hard a...  \n",
       "2  although realize that principle your strongest...  \n",
       "3  notwithstanding legitimate fuss about this pro...  \n",
       "4  well will have change scoring playoff pool unf...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"document\":documents})\n",
    "# 알파벳만 남기고 제거\n",
    "df[\"clean_document\"] = df[\"document\"].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어 제거 > 왱? 짧은 단어는 유용한 정보를 담고있지 않다고 가정한다는군\n",
    "df[\"clean_document\"] = df[\"clean_document\"].apply(lambda x: \" \".join([w for w in x.split() if len(w) > 3]))\n",
    "# 소문자로 변환\n",
    "df[\"clean_document\"] = df[\"clean_document\"].apply(lambda x: x.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:05:23.997566Z",
     "start_time": "2020-09-02T13:05:23.993636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well sure about story seem biased what disagree with your statement that media ruin israels reputation that rediculous media most israeli media world having lived europe realize that incidences such described letter have occured media whole seem ignore them subsidizing israels existance europeans least same degree think that might reason they report more clearly atrocities what shame that austria daily reports inhuman acts commited israeli soldiers blessing received from government makes some holocaust guilt away after look jews treating other races when they power unfortunate'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_document\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:07:00.545013Z",
     "start_time": "2020-09-02T13:07:00.541208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stopwords = stopwords.words('english')\n",
    "len(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:08:52.974461Z",
     "start_time": "2020-09-02T13:08:50.789371Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well',\n",
       " 'sure',\n",
       " 'about',\n",
       " 'story',\n",
       " 'seem',\n",
       " 'biased',\n",
       " 'what',\n",
       " 'disagree',\n",
       " 'with',\n",
       " 'your',\n",
       " 'statement',\n",
       " 'that',\n",
       " 'media',\n",
       " 'ruin',\n",
       " 'israels',\n",
       " 'reputation',\n",
       " 'that',\n",
       " 'rediculous',\n",
       " 'media',\n",
       " 'most',\n",
       " 'israeli',\n",
       " 'media',\n",
       " 'world',\n",
       " 'having',\n",
       " 'lived',\n",
       " 'europe',\n",
       " 'realize',\n",
       " 'that',\n",
       " 'incidences',\n",
       " 'such',\n",
       " 'described',\n",
       " 'letter',\n",
       " 'have',\n",
       " 'occured',\n",
       " 'media',\n",
       " 'whole',\n",
       " 'seem',\n",
       " 'ignore',\n",
       " 'them',\n",
       " 'subsidizing',\n",
       " 'israels',\n",
       " 'existance',\n",
       " 'europeans',\n",
       " 'least',\n",
       " 'same',\n",
       " 'degree',\n",
       " 'think',\n",
       " 'that',\n",
       " 'might',\n",
       " 'reason',\n",
       " 'they',\n",
       " 'report',\n",
       " 'more',\n",
       " 'clearly',\n",
       " 'atrocities',\n",
       " 'what',\n",
       " 'shame',\n",
       " 'that',\n",
       " 'austria',\n",
       " 'daily',\n",
       " 'reports',\n",
       " 'inhuman',\n",
       " 'acts',\n",
       " 'commited',\n",
       " 'israeli',\n",
       " 'soldiers',\n",
       " 'blessing',\n",
       " 'received',\n",
       " 'from',\n",
       " 'government',\n",
       " 'makes',\n",
       " 'some',\n",
       " 'holocaust',\n",
       " 'guilt',\n",
       " 'away',\n",
       " 'after',\n",
       " 'look',\n",
       " 'jews',\n",
       " 'treating',\n",
       " 'other',\n",
       " 'races',\n",
       " 'when',\n",
       " 'they',\n",
       " 'power',\n",
       " 'unfortunate']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokenize_document\"] = df[\"clean_document\"].apply(lambda x: [w for w in x.split() if x not in english_stopwords])\n",
    "df[\"tokenize_document\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:10:43.874835Z",
     "start_time": "2020-09-02T13:10:43.832524Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenized_doc = df[\"tokenize_document\"].apply(lambda x: \" \".join(x))\n",
    "len(detokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:10:44.468238Z",
     "start_time": "2020-09-02T13:10:44.464965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well sure about story seem biased what disagree with your statement that media ruin israels reputation that rediculous media most israeli media world having lived europe realize that incidences such described letter have occured media whole seem ignore them subsidizing israels existance europeans least same degree think that might reason they report more clearly atrocities what shame that austria daily reports inhuman acts commited israeli soldiers blessing received from government makes some holocaust guilt away after look jews treating other races when they power unfortunate'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenized_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:12:00.114827Z",
     "start_time": "2020-09-02T13:12:00.109032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000,\n",
    "    max_df=0.5,\n",
    "    smooth_idf=True\n",
    ")\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:12:31.832925Z",
     "start_time": "2020-09-02T13:12:30.974870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(detokenized_doc)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:23:34.431504Z",
     "start_time": "2020-09-02T13:23:34.427540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=20, n_iter=100,\n",
       "       random_state=0, tol=0.0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_svd = TruncatedSVD(\n",
    "    n_components=20,\n",
    "    algorithm=\"randomized\",\n",
    "    n_iter=100,\n",
    "    random_state=0\n",
    ")\n",
    "trunc_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:23:35.471920Z",
     "start_time": "2020-09-02T13:23:34.469025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_svd.fit(X)\n",
    "len(trunc_svd.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:23:52.554266Z",
     "start_time": "2020-09-02T13:23:52.546656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TruncatedSVD in module sklearn.decomposition.truncated_svd object:\n",
      "\n",
      "class TruncatedSVD(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      " |  TruncatedSVD(n_components=2, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n",
      " |  \n",
      " |  Dimensionality reduction using truncated SVD (aka LSA).\n",
      " |  \n",
      " |  This transformer performs linear dimensionality reduction by means of\n",
      " |  truncated singular value decomposition (SVD). Contrary to PCA, this\n",
      " |  estimator does not center the data before computing the singular value\n",
      " |  decomposition. This means it can work with scipy.sparse matrices\n",
      " |  efficiently.\n",
      " |  \n",
      " |  In particular, truncated SVD works on term count/tf-idf matrices as\n",
      " |  returned by the vectorizers in sklearn.feature_extraction.text. In that\n",
      " |  context, it is known as latent semantic analysis (LSA).\n",
      " |  \n",
      " |  This estimator supports two algorithms: a fast randomized SVD solver, and\n",
      " |  a \"naive\" algorithm that uses ARPACK as an eigensolver on (X * X.T) or\n",
      " |  (X.T * X), whichever is more efficient.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <LSA>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_components : int, default = 2\n",
      " |      Desired dimensionality of output data.\n",
      " |      Must be strictly less than the number of features.\n",
      " |      The default value is useful for visualisation. For LSA, a value of\n",
      " |      100 is recommended.\n",
      " |  \n",
      " |  algorithm : string, default = \"randomized\"\n",
      " |      SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n",
      " |      (scipy.sparse.linalg.svds), or \"randomized\" for the randomized\n",
      " |      algorithm due to Halko (2009).\n",
      " |  \n",
      " |  n_iter : int, optional (default 5)\n",
      " |      Number of iterations for randomized SVD solver. Not used by ARPACK.\n",
      " |      The default is larger than the default in `randomized_svd` to handle\n",
      " |      sparse matrices that may have large slowly decaying spectrum.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default = None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  tol : float, optional\n",
      " |      Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n",
      " |      SVD solver.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  components_ : array, shape (n_components, n_features)\n",
      " |  \n",
      " |  explained_variance_ : array, shape (n_components,)\n",
      " |      The variance of the training samples transformed by a projection to\n",
      " |      each component.\n",
      " |  \n",
      " |  explained_variance_ratio_ : array, shape (n_components,)\n",
      " |      Percentage of variance explained by each of the selected components.\n",
      " |  \n",
      " |  singular_values_ : array, shape (n_components,)\n",
      " |      The singular values corresponding to each of the selected components.\n",
      " |      The singular values are equal to the 2-norms of the ``n_components``\n",
      " |      variables in the lower-dimensional space.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.decomposition import TruncatedSVD\n",
      " |  >>> from sklearn.random_projection import sparse_random_matrix\n",
      " |  >>> X = sparse_random_matrix(100, 100, density=0.01, random_state=42)\n",
      " |  >>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
      " |  >>> svd.fit(X)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  TruncatedSVD(algorithm='randomized', n_components=5, n_iter=7,\n",
      " |          random_state=42, tol=0.0)\n",
      " |  >>> print(svd.explained_variance_ratio_)  # doctest: +ELLIPSIS\n",
      " |  [0.0606... 0.0584... 0.0497... 0.0434... 0.0372...]\n",
      " |  >>> print(svd.explained_variance_ratio_.sum())  # doctest: +ELLIPSIS\n",
      " |  0.249...\n",
      " |  >>> print(svd.singular_values_)  # doctest: +ELLIPSIS\n",
      " |  [2.5841... 2.5245... 2.3201... 2.1753... 2.0443...]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  PCA\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  Finding structure with randomness: Stochastic algorithms for constructing\n",
      " |  approximate matrix decompositions\n",
      " |  Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  SVD suffers from a problem called \"sign indeterminacy\", which means the\n",
      " |  sign of the ``components_`` and the output from transform depend on the\n",
      " |  algorithm and random state. To work around this, fit instances of this\n",
      " |  class to data once, then keep the instance around to do transformations.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TruncatedSVD\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_components=2, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit LSI model on training data X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns the transformer object.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit LSI model to X and perform dimensionality reduction on X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array, shape (n_samples, n_components)\n",
      " |          Reduced version of X. This will always be a dense array.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Transform X back to its original space.\n",
      " |      \n",
      " |      Returns an array X_original whose transform would be X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_components)\n",
      " |          New data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_original : array, shape (n_samples, n_features)\n",
      " |          Note that this is always a dense array.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Perform dimensionality reduction on X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          New data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array, shape (n_samples, n_components)\n",
      " |          Reduced version of X. This will always be a dense array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trunc_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:25:10.425254Z",
     "start_time": "2020-09-02T13:25:10.421286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20,), array([17.16,  9.94,  8.17,  7.92,  7.63,  7.53,  7.25,  7.01,  6.88,\n",
       "         6.86,  6.68,  6.56,  6.53,  6.42,  6.34,  6.22,  6.17,  6.09,\n",
       "         6.  ,  5.91]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s\n",
    "trunc_svd.singular_values_.shape, trunc_svd.singular_values_.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:24:44.575257Z",
     "start_time": "2020-09-02T13:24:44.571992Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V\n",
    "trunc_svd.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:31:19.862571Z",
     "start_time": "2020-09-02T13:31:19.853888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('just', 0.20887), ('like', 0.20469), ('know', 0.19349), ('people', 0.18318), ('think', 0.1697)]\n",
      "Topic 2: [('thanks', 0.32763), ('windows', 0.28786), ('card', 0.18019), ('drive', 0.16864), ('mail', 0.15261)]\n",
      "Topic 3: [('game', 0.34011), ('team', 0.30311), ('year', 0.26894), ('games', 0.23784), ('drive', 0.17472)]\n",
      "Topic 4: [('drive', 0.46159), ('scsi', 0.17188), ('disk', 0.14451), ('hard', 0.13805), ('problem', 0.12763)]\n",
      "Topic 5: [('drive', 0.39993), ('know', 0.28768), ('thanks', 0.24917), ('does', 0.24678), ('just', 0.17387)]\n",
      "Topic 6: [('just', 0.55559), ('like', 0.23559), ('windows', 0.23078), ('know', 0.15795), ('does', 0.11156)]\n",
      "Topic 7: [('just', 0.43264), ('like', 0.22858), ('mail', 0.15052), ('bike', 0.11698), ('thanks', 0.10025)]\n",
      "Topic 8: [('does', 0.39692), ('know', 0.25192), ('chip', 0.22492), ('like', 0.17824), ('card', 0.15695)]\n",
      "Topic 9: [('like', 0.42065), ('card', 0.32249), ('sale', 0.20267), ('video', 0.1571), ('offer', 0.14119)]\n",
      "Topic 10: [('like', 0.61166), ('drive', 0.23998), ('file', 0.13257), ('files', 0.09233), ('sounds', 0.08533)]\n",
      "Topic 11: [('people', 0.44189), ('like', 0.25647), ('thanks', 0.19072), ('card', 0.18615), ('government', 0.18341)]\n",
      "Topic 12: [('think', 0.66867), ('good', 0.25984), ('thanks', 0.11249), ('need', 0.10479), ('chip', 0.09178)]\n",
      "Topic 13: [('think', 0.36273), ('does', 0.22264), ('just', 0.21194), ('mail', 0.12875), ('like', 0.12095)]\n",
      "Topic 14: [('know', 0.36546), ('good', 0.30975), ('people', 0.27825), ('windows', 0.2729), ('file', 0.20088)]\n",
      "Topic 15: [('space', 0.4519), ('know', 0.31141), ('think', 0.18571), ('nasa', 0.1711), ('card', 0.11724)]\n",
      "Topic 16: [('does', 0.42324), ('israel', 0.31948), ('think', 0.26996), ('right', 0.1863), ('israeli', 0.17007)]\n",
      "Topic 17: [('good', 0.41859), ('space', 0.27501), ('card', 0.1855), ('does', 0.16985), ('thanks', 0.16633)]\n",
      "Topic 18: [('people', 0.5173), ('does', 0.25965), ('window', 0.21657), ('problem', 0.19057), ('space', 0.12958)]\n",
      "Topic 19: [('right', 0.36578), ('bike', 0.31154), ('time', 0.19438), ('windows', 0.18916), ('space', 0.1777)]\n",
      "Topic 20: [('file', 0.53334), ('problem', 0.20198), ('files', 0.19583), ('need', 0.18333), ('time', 0.15802)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(f\"Topic {idx+1}: {[(feature_names[i], topic[i].round(5)) for i in np.argsort(topic)[::-1][:n]]}\")\n",
    "        \n",
    "get_topics(trunc_svd.components_, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
